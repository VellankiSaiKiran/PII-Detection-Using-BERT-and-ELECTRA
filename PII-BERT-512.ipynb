{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2f0540be86564ebda3556b6ff7d75015":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_69f46b465e3c45ba86f89ec4deabd062","IPY_MODEL_0554de5fdd814e3ca7518eaa1b62c949","IPY_MODEL_15d05d65bbed49db825ef9551b0ab460"],"layout":"IPY_MODEL_e3aa326f6d034214880a7763a3de5726"}},"69f46b465e3c45ba86f89ec4deabd062":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41e6588343d54e8db50b5fb846ddb0ab","placeholder":"​","style":"IPY_MODEL_5dd62462f8984995a6c383ec79360aad","value":"model.safetensors: 100%"}},"0554de5fdd814e3ca7518eaa1b62c949":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a66d0737d3b465994677c69b8bf9f09","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f105797de38e435e989f2a118fe65bf7","value":440449768}},"15d05d65bbed49db825ef9551b0ab460":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b220e5beacd9487bbe28a12b69c3134c","placeholder":"​","style":"IPY_MODEL_e2145a19dd2c4abc80a07cb635624c0e","value":" 440M/440M [00:01&lt;00:00, 408MB/s]"}},"e3aa326f6d034214880a7763a3de5726":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41e6588343d54e8db50b5fb846ddb0ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dd62462f8984995a6c383ec79360aad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a66d0737d3b465994677c69b8bf9f09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f105797de38e435e989f2a118fe65bf7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b220e5beacd9487bbe28a12b69c3134c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2145a19dd2c4abc80a07cb635624c0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["This Python notebook serves as a technical companion to our in-depth blog post discussing the challenges associated with managing personally identifiable information (PII) in educational datasets.\n","\n","It focuses on the BERT (Bidirectional Encoder Representations from Transformers) machine learning model, which **provided the most satisfactory results** in our evaluation. Here, we detail the implementation process, evaluate the performance, and offer comprehensive insights into how the BERT model effectively detects and anonymizes PII, ensuring the privacy and security of student data.\n","\n","The notebook includes practical code snippets, performance analysis, and explanations to aid in understanding the application of BERT in this critical context."],"metadata":{"id":"VA5--1Dex1JS"}},{"cell_type":"markdown","source":["**Required Imports**"],"metadata":{"id":"sL6BkuCPx7dw"}},{"cell_type":"code","source":["# Required Imports\n","import pandas as pd\n","import re\n","import ast\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from transformers import BertTokenizerFast, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n","import numpy as np\n","import torch.nn as nn\n","from sklearn.metrics import classification_report\n","from google.colab import drive\n","from torch import nn\n","import random"],"metadata":{"id":"BoCpuElwx5Qt","executionInfo":{"status":"ok","timestamp":1714406484044,"user_tz":240,"elapsed":7620,"user":{"displayName":"Sai Kiran Reddy Vellanki","userId":"14861279026824216393"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["**Data Loading**\n","\n","Data can be downloaded from: [Kaggle Pii Data Detection Dataset](https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/data )"],"metadata":{"id":"4UZ1IEhqyBBV"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","file_path = '/content/drive/My Drive/ML-Project/PII-DATA'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yiYgWk8m3FpL","outputId":"7dad04df-2681-488d-df7b-b613ece3b6ff","executionInfo":{"status":"ok","timestamp":1714406531066,"user_tz":240,"elapsed":47042,"user":{"displayName":"Sai Kiran Reddy Vellanki","userId":"14861279026824216393"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"Z2nBRXeE2-16","outputId":"030c947e-e1c7-4cc8-ae20-773a5c781a70","executionInfo":{"status":"ok","timestamp":1714406536822,"user_tz":240,"elapsed":5759,"user":{"displayName":"Sai Kiran Reddy Vellanki","userId":"14861279026824216393"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       document                                              words  \\\n","0             7  ['Design', 'Thinking', 'for', 'innovation', 'r...   \n","1            10  ['Diego', 'Estrada', '\\n\\n', 'Design', 'Thinki...   \n","2            16  ['Reporting', 'process', '\\n\\n', 'by', 'Gilber...   \n","3            20  ['Design', 'Thinking', 'for', 'Innovation', '\\...   \n","4            56  ['Assignment', ':', '\\xa0 ', 'Visualization', ...   \n","...         ...                                                ...   \n","11236     29429  ['Hello,', \"I'm\", 'Nicholas', 'Moore,', 'a', '...   \n","11237     29430  ['Hello,', 'my', 'name', 'is', 'Alexey', 'Novi...   \n","11238     29431  ['My', 'name', 'is', 'Ludmila', 'Inoue,', 'and...   \n","11239     29432  ['Dr.', 'Tu', 'Garcia,', 'a', 'renowned', 'der...   \n","11240     29433  ['Hello,', \"I'm\", 'Badi', 'Nakamura,', 'and', ...   \n","\n","                                                  labels  \n","0      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n","1      ['B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O',...  \n","2      ['O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME...  \n","3      ['O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I...  \n","4      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n","...                                                  ...  \n","11236  ['O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT',...  \n","11237  ['O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME...  \n","11238  ['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUD...  \n","11239  ['O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O',...  \n","11240  ['O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT',...  \n","\n","[11241 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-3ac3bca9-9634-4580-b167-212ebb2df529\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>words</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7</td>\n","      <td>['Design', 'Thinking', 'for', 'innovation', 'r...</td>\n","      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10</td>\n","      <td>['Diego', 'Estrada', '\\n\\n', 'Design', 'Thinki...</td>\n","      <td>['B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O',...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>16</td>\n","      <td>['Reporting', 'process', '\\n\\n', 'by', 'Gilber...</td>\n","      <td>['O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>20</td>\n","      <td>['Design', 'Thinking', 'for', 'Innovation', '\\...</td>\n","      <td>['O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>56</td>\n","      <td>['Assignment', ':', '\\xa0 ', 'Visualization', ...</td>\n","      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11236</th>\n","      <td>29429</td>\n","      <td>['Hello,', \"I'm\", 'Nicholas', 'Moore,', 'a', '...</td>\n","      <td>['O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT',...</td>\n","    </tr>\n","    <tr>\n","      <th>11237</th>\n","      <td>29430</td>\n","      <td>['Hello,', 'my', 'name', 'is', 'Alexey', 'Novi...</td>\n","      <td>['O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME...</td>\n","    </tr>\n","    <tr>\n","      <th>11238</th>\n","      <td>29431</td>\n","      <td>['My', 'name', 'is', 'Ludmila', 'Inoue,', 'and...</td>\n","      <td>['O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUD...</td>\n","    </tr>\n","    <tr>\n","      <th>11239</th>\n","      <td>29432</td>\n","      <td>['Dr.', 'Tu', 'Garcia,', 'a', 'renowned', 'der...</td>\n","      <td>['O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O',...</td>\n","    </tr>\n","    <tr>\n","      <th>11240</th>\n","      <td>29433</td>\n","      <td>['Hello,', \"I'm\", 'Badi', 'Nakamura,', 'and', ...</td>\n","      <td>['O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT',...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11241 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ac3bca9-9634-4580-b167-212ebb2df529')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3ac3bca9-9634-4580-b167-212ebb2df529 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3ac3bca9-9634-4580-b167-212ebb2df529');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-50ce4ff4-0560-49e7-b9eb-ce1991c6fb63\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-50ce4ff4-0560-49e7-b9eb-ce1991c6fb63')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-50ce4ff4-0560-49e7-b9eb-ce1991c6fb63 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_6b6db387-7ce7-47d1-9cf5-140b83a8b2de\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_6b6db387-7ce7-47d1-9cf5-140b83a8b2de button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 11241,\n  \"fields\": [\n    {\n      \"column\": \"document\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7245,\n        \"min\": 7,\n        \"max\": 29433,\n        \"num_unique_values\": 11241,\n        \"samples\": [\n          16696,\n          25160,\n          5910\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"words\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11241,\n        \"samples\": [\n          \"['Reflection', '\\u2013', 'Mind', 'Mapping', '\\\\n\\\\n', 'Challenge', '\\\\n\\\\n', 'An', 'insurance', 'client', 'of', 'mine', 'had', 'wanted', 'to', 'improve', 'the', 'efficiency', 'of', 'its', 'planning', 'and', 'response', 'operations', ' ', 'following', 'catastrophe', 'events', '.', 'It', 'takes', 'coordination', 'and', 'collaboration', 'amongst', 'multi', '-', 'functional', 'teams', ' ', 'such', 'as', 'weather', 'watchers', ',', 'logistics', ',', 'deployment', ',', 'dispatch', ',', 'and', 'field', 'personnel', 'to', 'respond', 'and', 'attend', 'to', ' ', 'its', 'customers', 'in', 'time', 'of', 'need', '.', 'Though', 'the', 'teams', 'worked', 'hard', 'and', 'gave', 'their', 'fullest', ',', 'the', 'customer', ' ', 'satisfaction', 'score', '(', 'CSAT', ')', 'was', 'negatively', 'trending', 'for', 'the', 'last', '6', 'months', '.', 'After', 'interviewing', 'several', ' ', 'members', 'of', 'the', 'teams', ',', 'it', 'became', 'apparent', 'that', 'while', 'they', 'knew', 'what', 'their', 'specific', 'challenges', 'were', 'and', ' ', 'even', 'what', 'possible', 'solutions', 'are', ',', 'we', 'needed', 'to', 'step', 'back', 'and', 'develop', 'cross', '-', 'functional', 'insights', 'and', ' ', 'identify', 'patterns', 'on', 'current', 'reality', 'first', ',', 'i.e.', 'What', 'is', '?', '\\\\n\\\\n', 'Selection', '\\\\n\\\\n', 'Before', 'we', 'can', 'begin', 'generating', 'ideas', 'and', 'solutions', 'to', 'improve', 'the', 'operations', 'and', 'thus', 'the', 'CSAT', ',', 'the', ' ', 'stakeholders', 'needed', 'to', '\\u2018', 'map', 'their', 'minds', '\\u2019', 'and', 'establish', 'the', 'design', 'criteria', 'together', '.', 'We', 'used', 'the', 'mind', ' ', 'mapping', 'design', 'thinking', 'tool', 'to', 'accomplish', 'this', '.', '\\\\n\\\\n', 'Application', '\\\\n\\\\n', 'In', 'preparation', 'for', 'the', 'mind', 'mapping', 'exercise', ',', 'we', 'had', 'previously', 'created', 'personas', 'of', 'team', 'members', 'in', ' ', 'each', 'of', 'the', 'departments', '.', 'We', 'had', 'also', 'collected', 'quantitative', 'data', 'around', 'the', 'operations', 'such', 'as', 'volume', ' ', 'handled', 'per', 'day', ',', 'peak', 'and', 'trough', 'staff', 'strength', ',', 'technology', 'adoption', ',', 'and', 'workflow', '.', ' ', 'In', 'a', 'large', ' ', 'conference', 'room', ',', 'posters', 'with', 'information', 'gathered', 'were', 'affixed', 'on', 'the', 'walls', '.', 'Stakeholders', 'from', 'each', ' ', 'department', 'participated', 'in', 'the', 'exercise', '.', 'Line', 'managers', 'and', 'Senior', 'Executives', 'were', 'also', 'allowed', 'to', 'come', ' ', 'in', 'and', 'participate', '.', 'Participants', 'used', 'index', 'cards', 'to', 'write', 'down', 'their', 'viewpoints', 'on', 'operational', 'challenges', ' ', 'faced', '.', 'We', 'then', 'helped', 'the', 'team', 'identify', '5', 'broad', 'themes', 'and', 'created', 'clusters', 'of', 'index', 'cards', 'around', 'the', ' ', 'themes', '.', 'Outliers', 'were', 'recorded', 'and', 'were', 'n\\u2019t', 'ignored', '.', '\\\\n\\\\n', 'Insight', '\\\\n\\\\n', 'One', 'of', 'the', 'immediate', 'outcomes', 'of', 'the', 'exercise', 'was', 'both', 'enlightenment', 'and', 'appreciation', 'of', 'work', ' ', 'performed', 'by', 'each', 'department', 'and', 'how', 'it', 'collectively', 'helps', 'serving', 'their', 'customers', '.', 'Second', ',', 'challenges', ' ', 'as', 'noted', 'by', 'one', 'department', 'took', 'on', 'whole', 'new', 'implications', 'when', 'considered', 'in', 'the', 'larger', 'scheme', 'of', ' ', 'operations', '.', 'As', 'an', 'example', ',', 'the', 'logistics', 'team', 'was', 'planning', 'deployments', 'without', 'up', 'to', 'date', 'information', '\\\\n\\\\n', 'on', 'current', 'whereabouts', 'of', 'field', 'personnel', '.', 'We', 'further', 'had', 'a', 'lot', 'of', 'input', 'from', 'each', 'department', 'that', 'led', ' ', 'of', 'more', 'accurate', 'description', 'of', 'problem', 'statements', '.', 'Third', ',', 'a', 'number', 'of', 'tactical', 'items', 'were', 'resolved', ' ', 'through', 'better', 'understanding', 'of', 'the', 'current', 'reality', '.', 'These', 'were', 'quick', '-', 'hit', 'solutions', 'and', 'helped', 'with', 'the', ' ', 'momentum', 'of', 'the', 'exercise', '.', '\\\\n\\\\n', 'In', 'summary', ',', 'I', 'thought', 'the', 'mind', 'mapping', 'technique', 'really', 'engaged', 'the', 'stakeholders', 'and', 'made', 'them', ' ', 'realize', 'that', 'inter', '-', 'departmental', 'solutions', 'are', 'needed', 'to', 'address', 'their', 'business', 'problem', 'as', 'opposed', 'to', ' ', 'isolated', 'initiatives', '.', 'It', 'also', 'helped', 'us', 'to', 'focus', 'on', '5', 'main', 'themes', 'namely', ',', 'operating', 'model', ',', 'technology', ' ', 'upgrade', ',', 'process', 'streamlining', ',', 'customer', 'experience', 'and', 'employee', 'engagement', '/', 'training', 'which', 'were', ' ', 'then', 'used', 'as', 'a', 'basis', 'for', 'brainstorming', 'to', 'generate', 'ideas', '.', '\\\\n\\\\n', 'Approach', '\\\\n\\\\n', 'I', 'will', 'continue', 'to', 'use', 'the', 'mind', 'mapping', 'technique', 'to', 'transition', 'from', 'the', 'definition', 'of', 'problem(s', ')', 'to', ' ', 'developing', 'ideas', 'to', 'solve', 'them', '.', 'There', 'was', 'a', 'strong', 'but', 'understandable', 'inclination', 'by', 'the', 'participants', ' ', 'towards', 'solutioning', 'the', 'problem', 'rather', 'than', 'establishing', 'the', 'design', 'criteria', '.', 'Through', 'the', 'experience', ' ', 'gained', ',', 'I', 'will', 'be', 'better', 'able', 'to', 'conduct', 'the', 'exercise', 'by', 'impressing', 'up', 'on', 'the', 'need', 'to', 'establish', '\\u2018', 'What', 'is', '\\u2019', ' ', 'before', 'moving', 'on', 'to', '\\u2018', 'What', 'if', '\\u2019', '.', 'Also', ',', 'I', 'would', 'employ', 'the', '\\u2018', 'So', 'What', '\\u2019', 'question', 'in', 'the', 'future', 'more', 'than', 'I', 'did', ' ', 'for', 'the', 'exercise', 'above', 'to', 'help', 'the', 'stakeholders', 'glean', 'insights', 'than', 'mere', 'observations', '.', '\\\\n\\\\n']\",\n          \"['Hi,', 'my', 'name', 'is', 'Carmen', 'Chen', 'and', 'I', 'work', 'as', 'an', 'Industrial', 'Engineer.', 'I', 'was', 'faced', 'with', 'a', 'unique', 'challenge', 'during', 'my', 'time', 'at', 'Smith', 'and', 'Jones', 'Manufacturing.', 'The', 'company', 'had', 'been', 'struggling', 'with', 'a', 'significant', 'amount', 'of', 'waste', 'in', 'their', 'production', 'process,', 'leading', 'to', 'increased', 'costs', 'and', 'environmental', 'concerns.', 'The', 'main', 'issue', 'was', 'that', 'the', 'machines', 'on', 'the', 'assembly', 'line', 'were', 'not', 'properly', 'synchronized,', 'resulting', 'in', 'frequent', 'stoppages', 'and', 'delays.', 'This', 'was', 'compounded', 'by', 'the', 'lack', 'of', 'communication', 'between', 'different', 'departments,', 'leading', 'to', 'further', 'inefficiencies', 'and', 'rework.', 'Additionally,', 'there', 'were', 'several', 'inefficiencies', 'in', 'the', 'material', 'handling', 'and', 'storage', 'systems,', 'causing', 'delays', 'and', 'bottlenecks.', 'To', 'address', 'these', 'issues,', 'I', 'conducted', 'a', 'thorough', 'analysis', 'of', 'the', 'production', 'process,', 'identifying', 'the', 'root', 'causes', 'of', 'the', 'problems.', 'I', 'redesigned', 'the', 'assembly', 'line', 'layout,', 'optimizing', 'the', 'flow', 'of', 'materials', 'and', 'products.', 'I', 'also', 'implemented', 'a', 'new', 'inventory', 'management', 'system', 'that', 'provided', 'real-time', 'updates', 'on', 'material', 'availability', 'and', 'usage,', 'reducing', 'the', 'chances', 'of', 'stockouts', 'and', 'overstocking.', 'Furthermore,', 'I', 'developed', 'a', 'comprehensive', 'training', 'program', 'to', 'improve', 'communication', 'and', 'coordination', 'among', 'different', 'departments,', 'fostering', 'a', 'culture', 'of', 'teamwork', 'and', 'collaboration.', 'Through', 'these', 'changes,', 'we', 'were', 'able', 'to', 'significantly', 'reduce', 'waste,', 'improve', 'productivity,', 'and', 'enhance', 'the', 'overall', 'efficiency', 'of', 'the', 'production', 'process.', 'As', 'a', 'result,', 'Smith', 'and', 'Jones', 'Manufacturing', 'experienced', 'substantial', 'cost', 'savings', 'and', 'increased', 'profitability.', 'If', \\\"you'd\\\", 'like', 'to', 'contact', 'me,', 'my', 'email', 'is', 'carmen.chen2181@hotmail.net,', 'or', 'you', 'can', 'reach', 'me', 'at', 'my', 'address:', '3504', 'East', '5th', 'Street.']\",\n          \"['Federation', '\\\\n\\\\n', 'By', 'Sara', 'Garcia', '\\\\n\\\\n', 'Challenge', '\\\\n\\\\n', 'With', 'a', 'history', 'of', 'working', 'on', 'multiple', 'projects', 'I', '(', 'and', 'assuming', 'many', 'others', ')', 'have', 'faced', '\\\\xa0 ', 'a', 'very', 'peculiar', 'conundrum', 'throughout', 'a', 'project', '\\u2019s', 'creation', 'lifecycle', '.', '\\\\n\\\\n', 'Whenever', 'a', 'new', 'project', 'is', 'executed', ',', 'post', 'an', 'initial', 'meeting', ',', 'every', 'team', 'scatters', 'and', '\\\\xa0 ', 'starts', 'ideating', 'their', 'own', 'version', 'of', 'the', 'project', 'which', 'ideally', 'roots', 'out', 'and', 'grows', 'from', '\\\\n\\\\n', 'their', 'areas', 'of', 'expertise', '.', 'I.e', 'a', 'developer', 'would', 'ideate', 'from', 'a', 'developer', '\\u2019s', 'POV', 'focusing', '\\\\n\\\\n', 'more', 'on', 'the', 'technical', 'aspects', 'and', 'a', 'Business', 'developer', 'would', 'do', 'it', 'from', 'their', 'POV', '\\\\n\\\\n', 'focusing', 'on', 'business', 'goals', 'etc', '.', '\\\\n\\\\n', 'The', 'challenge', 'here', 'is', 'to', 'ensure', 'that', 'every', 'team', 'member', 'involved', 'in', 'the', 'project', 'has', 'a', '\\\\xa0 ', 'common', 'understanding', 'of', 'the', 'project', 'regardless', 'of', 'their', 'involvement', 'at', 'any', 'stage', 'of', '\\\\n\\\\n', 'the', 'project', '.', '\\\\n\\\\n', 'The', 'solution', 'to', 'this', 'challenge', 'seems', 'simple', 'on', 'the', 'outside', 'yet', 'precarious', 'when', 'tackled', '.', '\\\\n\\\\n', 'Selection', '\\\\n\\\\n', 'The', 'tool', 'selected', 'for', 'this', 'challenge', 'is', 'Mind', 'mapping', ',', 'due', 'to', 'its', 'ability', 'to', 'visually', '\\\\xa0 ', 'communicate', 'large', 'chunks', 'of', 'information', 'in', 'an', 'organised', 'manner', '.', 'The', 'tool', 'would', 'free', '\\\\n\\\\n', 'up', 'useful', 'time', 'which', 'would', 'otherwise', 'be', 'wasted', 'explaining', 'all', 'the', 'information', 'to', 'bring', '\\\\n\\\\n', 'people', 'up', 'to', 'speed', '.', '\\\\n\\\\n', 'Application', '\\\\n\\\\n', 'The', 'approach', 'was', 'to', 'create', 'multiple', 'mind', 'maps', 'at', 'various', 'stages', 'which', 'would', '\\\\xa0 ', 'showcase', 'the', 'entire', 'project', 'journey', 'and', 'ensure', 'any', 'participant', 'who', 'would', 'join', 'midway', '\\\\n\\\\n', 'in', 'the', 'project', 'would', 'catch', 'up', 'with', 'the', 'least', 'amount', 'of', 'time', 'wasted', ',', 'this', 'would', 'also', '\\\\n\\\\n', 'ensure', 'that', 'each', 'member', 'is', 'in', 'sync', 'with', 'the', 'final', 'goal', 'of', 'the', 'project', 'as', 'well', 'while', 'being', '\\\\n\\\\n', 'able', 'to', 'see', 'a', 'broad', 'view', 'of', 'the', 'project', 'to', 'provide', 'better', 'ideas', 'and', 'spot', 'potential', 'issues', '.', '\\\\n\\\\n', 'Once', 'a', 'project', 'was', 'initiated', ',', 'An', 'empty', 'room', 'was', 'selected', 'and', 'a', '\\\\u200bclient', 'mind', 'map\\\\u200b', 'was', '\\\\xa0 ', 'created', 'which', 'contained', 'all', 'of', 'the', 'client', '\\u2019s', 'insights', '.', 'This', 'helped', 'the', 'design', 'team', 'process', '\\\\n\\\\n', 'the', 'information', 'faster', 'and', 'proceed', 'with', 'the', 'next', 'steps', 'of', 'ideation', '.', 'Post', 'the', 'ideation', '\\\\n\\\\n', 'session', 'another', 'mind', 'map', '(', '\\\\u200bideation', 'mind', 'map\\\\u200b', ')', 'was', 'created', 'to', 'showcase', 'a', 'refined', '\\\\n\\\\n', 'project', 'idea', 'which', 'incorporated', 'multiple', 'missing', 'facets', 'that', 'would', 'otherwise', 'impact', '\\\\n\\\\n', 'the', 'success', 'of', 'the', 'project', '.', 'Post', 'ideation', 'was', 'the', 'research', 'and', 'design', ',', 'we', 'added', 'in', 'the', '\\\\n\\\\n', 'research', 'information', 'into', 'the', '\\u2018', 'ideation', 'mind', 'map', '\\u2019', 'and', 'the', 'result', 'of', 'the', 'design', 'team', 'was', '\\\\n\\\\n', 'a', 'set', 'of', 'wireframes', 'organised', 'as', 'app', 'flows', 'which', 'were', 'placed', 'beside', 'the', 'ideation', 'mind', '\\\\n\\\\n', 'map', 'and', 'showcased', 'the', 'resulting', 'output', 'of', 'the', 'project', '.', '\\\\n\\\\n', 'At', 'this', 'stage', 'the', 'client', ',', 'a', 'few', 'developers', ',', 'and', 'business', 'development', 'team', 'were', '\\\\xa0 ', 'brought', 'in', 'to', 'look', 'at', 'the', 'work', 'in', 'progress', '.', '\\\\n\\\\n', 'Rather', 'than', 'explaining', 'the', 'complete', 'process', 'and', 'work', 'done', 'by', 'the', 'design', 'team', ',', 'we', '\\\\xa0 ', 'asked', 'the', 'client', ',', 'developers', 'and', 'business', 'development', 'team', 'to', 'spend', 'a', 'few', 'minutes', '\\\\n\\\\n', 'looking', 'at', 'the', 'Client', 'mind', 'map', ',', 'then', 'the', 'ideation', 'mind', 'map', 'and', 'then', 'the', 'app', 'flow', '.', '\\\\n\\\\n', 'This', 'brought', 'all', 'the', 'stakeholders', 'of', 'the', 'project', 'in', 'sync', 'with', 'the', 'current', 'status', 'of', 'the', '\\\\xa0 ', 'project', ',', 'the', 'direction', 'and', 'saved', 'everyone', 'a', 'lot', 'of', 'time', '.', 'We', 'proceeded', 'doing', 'this', 'at', 'the', '\\\\n\\\\n', 'next', 'few', 'stages', 'of', 'the', 'design', 'and', 'development', 'cycles', 'to', 'ensure', 'the', 'complete', 'project', '\\\\n\\\\n', 'journey', 'is', 'captured', 'and', 'each', 'stakeholder', 'is', 'updated', 'with', 'the', 'project', 'status', 'and', '\\\\n\\\\n', 'direction', 'regardless', 'of', 'the', 'time', 'they', 'joined', 'the', 'project', '.', '\\\\n\\\\n', 'Insight', '\\\\n\\\\n', 'During', 'the', 'complete', 'process', 'of', 'this', 'experiment', 'I', 'noticed', 'increased', 'efficiency', ',', 'better', '\\\\xa0 ', 'idea', 'generation', 'and', 'a', 'unified', 'hive', 'style', 'thinking', 'approach', 'that', 'is', 'seen', 'when', '\\\\n\\\\n', 'participants', 'have', 'spent', 'close', 'to', '6', '-', '7', 'months', 'working', 'on', 'the', 'same', 'project', '.', '\\\\n\\\\n', 'We', 'were', 'also', 'able', 'to', 'generate', 'ideas', 'beyond', 'our', 'fields', 'of', 'expertise', 'and', 'also', 'catch', '\\\\xa0 ', 'issues', 'that', 'would', 'have', 'resulted', 'in', 'a', 'lot', 'of', 'rework', 'and', 'significant', 'loss', 'for', 'the', 'client', '.', '\\\\n\\\\n', 'We', 'had', 'a', 'few', 'participants', 'who', 'had', 'to', 'keep', 'shifting', 'from', 'project', 'to', 'project', 'throughout', '\\\\xa0 ', 'the', 'process', 'and', 'they', 'found', 'it', 'a', 'lot', 'easier', 'to', 'catch', 'up', 'with', 'the', 'progress', 'and', 'provide', '\\\\n\\\\n', 'useful', 'insights', 'during', 'the', 'time', 'they', 'spent', '.', '\\\\n\\\\n', 'Overall', ',', 'I', 'was', 'able', 'to', 'open', 'my', 'mind', 'and', 'think', 'from', 'various', 'other', 'perspectives', ',', 'in', 'a', 'way', '\\\\xa0 ', 'following', 'this', 'process', 'helped', 'us', 'to', 'get', 'out', 'of', 'our', 'metaphorical', 'boxes', ',', 'work', 'together', '\\\\n\\\\n', 'towards', 'a', 'common', 'goal', 'and', 'this', 'helped', 'us', 'provide', 'better', 'quality', 'to', 'our', 'clients', '.', '\\\\n\\\\n', 'Approach', '\\\\n\\\\n', 'Next', 'time', ',', 'I', 'would', 'probably', 'involve', 'a', 'few', 'additional', 'people', 'from', 'specific', 'fields', 'when', '\\\\xa0 ', 'constructing', 'and', 'app', 'of', 'a', 'particular', 'user', 'base', '.', 'Ex', '-', 'if', 'I', 'were', 'to', 'create', 'an', 'alternative', 'to', '\\\\n\\\\n', 'facebook', 'AD', 'manager', ',', 'I', 'would', 'do', 'my', 'research', ',', 'but', 'also', 'include', 'multiple', 'digital', '\\\\n\\\\n', 'marketers', 'to', 'the', 'team', 'and', 'create', 'a', 'custom', 'mind', 'map', 'of', 'their', 'thoughts', 'on', 'an', 'ideal', '\\\\n\\\\n', 'application', 'and', 'also', 'involve', 'them', 'in', 'ideation', 'sessions', 'to', 'help', 'refine', 'the', 'alternative', '\\\\n\\\\n', 'application', 'even', 'more', 'to', 'suit', 'its', 'end', 'users', '.', '\\\\n\\\\n', '----------Thanks', 'for', 'reading----------', '\\\\n\\\\n']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6641,\n        \"samples\": [\n          \"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\",\n          \"['O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-USERNAME', 'O', 'O', 'O', 'O', 'O', 'O', 'B-EMAIL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-URL_PERSONAL']\",\n          \"['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-URL_PERSONAL', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-USERNAME', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-STREET_ADDRESS', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":3}],"source":["import pandas as pd\n","\n","# Load the CSV data into a DataFrame\n","df = pd.read_csv('/content/drive/My Drive/ML-Project/PII-DATA/token-labels.csv')\n","\n","# Display the first few rows of the DataFrame to verify it's loaded correctly\n","df\n"]},{"cell_type":"code","source":["# Define a list of unique labels used in the dataset\n","unique_labels = [\n","    'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT', 'B-URL_PERSONAL', 'B-EMAIL',\n","    'B-ID_NUM', 'I-URL_PERSONAL', 'B-USERNAME', 'B-PHONE_NUM', 'I-PHONE_NUM',\n","    'B-STREET_ADDRESS', 'I-STREET_ADDRESS', 'I-ID_NUM'\n","]\n","\n","# Create a dictionary to map label indices to their respective labels\n","id2label = {\n","    0: 'O', 1: 'B-NAME_STUDENT', 2: 'I-NAME_STUDENT', 3: 'B-URL_PERSONAL',\n","    4: 'B-EMAIL', 5: 'B-ID_NUM', 6: 'I-URL_PERSONAL', 7: 'B-USERNAME',\n","    8: 'B-PHONE_NUM', 9: 'I-PHONE_NUM', 10: 'B-STREET_ADDRESS',\n","    11: 'I-STREET_ADDRESS', 12: 'I-ID_NUM'\n","}\n"],"metadata":{"id":"vgNS95FKQwpy","executionInfo":{"status":"ok","timestamp":1714406536823,"user_tz":240,"elapsed":7,"user":{"displayName":"Sai Kiran Reddy Vellanki","userId":"14861279026824216393"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# import ast\n","# df['words'] = df['words'].apply(ast.literal_eval)\n","# df['labels'] = df['labels'].apply(ast.literal_eval)\n"],"metadata":{"id":"zM-VpyaQYSE6","executionInfo":{"status":"ok","timestamp":1714406536823,"user_tz":240,"elapsed":5,"user":{"displayName":"Sai Kiran Reddy Vellanki","userId":"14861279026824216393"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["###Data Cleaning Functions"],"metadata":{"id":"HtNnPIS5bz4W"}},{"cell_type":"code","source":["import re\n","import ast\n","\n","def clean_words_and_labels(words, labels):\n","    \"\"\"\n","    Cleans words by removing whitespace and corresponding labels if the word is empty after cleaning.\n","    :param words: List of words to be cleaned.\n","    :param labels: Corresponding labels for the words.\n","    :return: Tuple of lists containing cleaned words and their corresponding labels.\n","    \"\"\"\n","    # Clean words and remove corresponding labels if the word is just whitespace or empty after cleaning.\n","    cleaned_words = []\n","    cleaned_labels = []\n","    for word, label in zip(words, labels):\n","        # Apply any specific cleaning necessary for each word\n","        cleaned_word = re.sub(r'\\s+', '', word).strip()\n","        # Only add words and labels that are not empty after cleaning\n","        if cleaned_word:\n","            cleaned_words.append(cleaned_word)\n","            cleaned_labels.append(label)\n","    return cleaned_words, cleaned_labels\n","\n","# Assuming 'words' and 'labels' columns contain string representations of lists\n","# Convert string representation of a list to actual list\n","df['words'] = df['words'].apply(ast.literal_eval)\n","df['labels'] = df['labels'].apply(ast.literal_eval)\n","\n","# Apply the cleaning function to each pair of words and labels\n","cleaned_data = df.apply(lambda row: clean_words_and_labels(row['words'], row['labels']), axis=1)\n","\n","# Separate the cleaned words and labels into their own columns\n","df['cleaned_words'] = cleaned_data.apply(lambda x: x[0])\n","df['cleaned_labels'] = cleaned_data.apply(lambda x: x[1])\n"],"metadata":{"id":"5s6x7IAQbdVQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.drop(columns=['words', 'labels'], inplace=True)"],"metadata":{"id":"KYJReUVubgt8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"mzaqv7Gr_YrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_quotes_from_cleaned_words(df):\n","    \"\"\"\n","    Removes quotes from each word in the 'cleaned_words' column of the DataFrame.\n","    :param df: DataFrame containing the 'cleaned_words' column.\n","    :return: DataFrame with quotes removed from words.\n","    \"\"\"\n","    # Removing quotes from each word in the 'cleaned_words' column\n","    df['cleaned_words'] = df['cleaned_words'].apply(lambda words: [word.replace('\"', '') for word in words])\n","    return df\n","\n","df = remove_quotes_from_cleaned_words(df)"],"metadata":{"id":"NOOk7-qiB4U7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def verify_and_display_alignment(df):\n","    # Check if each list of 'cleaned_words' has a corresponding list of 'cleaned_labels'\n","    for index, row in df.iterrows():\n","        words = row['cleaned_words']\n","        labels = row['cleaned_labels']\n","        if len(words) != len(labels):\n","            print(f\"Mismatch found in document {index}: {len(words)} words, {len(labels)} labels\")\n","            break\n","    else: # This else belongs to the for loop and executes if the loop doesn't break\n","        print(\"Each word has a corresponding label.\")\n","\n","    # Display words and labels for the first two 'sentences'\n","    for index, (words, labels) in enumerate(zip(df['cleaned_words'][2:5], df['cleaned_labels'][2:5])):\n","        print(f\"\\nSentence {index + 1}:\")\n","        for word, label in zip(words, labels):\n","            print(f\"{word}: {label}\")\n","\n","# Assuming 'df' is your dataframe and it contains 'cleaned_words' and 'cleaned_labels' columns\n","verify_and_display_alignment(df)\n"],"metadata":{"id":"xXEkSijE_c2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Explode the 'cleaned_labels' column and filter out 'O' which represent non-PII (Personally Identifiable Information) data before counting\n","label_counts = df['cleaned_labels'].explode().value_counts().drop('O', errors='ignore')\n","\n","# Plotting the label distribution without 'O'\n","plt.figure(figsize=(10, 6))\n","label_counts.plot(kind='bar')\n","plt.title('Distribution of PII Labels (Excluding Non-PII \"O\")')\n","plt.xlabel('Labels')\n","plt.ylabel('Frequency')\n","plt.xticks(rotation=45)\n","plt.show()\n"],"metadata":{"id":"H46EVULXzPQ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adding a new column to the DataFrame that represents the text length in terms of the number of words\n","df['text_length'] = df['cleaned_words'].apply(len)\n","\n","# Plotting the text length distribution\n","plt.figure(figsize=(10, 6))\n","plt.hist(df['text_length'], bins=50, color='blue', alpha=0.7)\n","plt.title('Distribution of Text Lengths')\n","plt.xlabel('Length of Text (number of words)')\n","plt.ylabel('Frequency')\n","plt.show()"],"metadata":{"id":"kSP3sFdazh_V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["non_o_word_label_pairs = []"],"metadata":{"id":"fnCtXqaQHVhh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_non_O_words_and_labels(df):\n","    # Get the last five entries in the DataFrame\n","    last_five = df.iloc[100:105]\n","\n","    # Iterate over each of the last five entries\n","    for index, (words, labels) in enumerate(zip(last_five['cleaned_words'], last_five['cleaned_labels'])):\n","        print(f\"Document {index + len(df) - 5}:\")\n","        # Create a list of tuples (word, label) where the label is not 'O'\n","        non_O_word_label_pairs = [(word, label) for word, label in zip(words, labels) if label != 'O']\n","\n","        # Print each word with its non-'O' label\n","        for word, label in non_O_word_label_pairs:\n","            print(f\"{word}: {label}\")\n","\n","# Call the function to print the words and their labels\n","# This should be run in your local environment\n","print_non_O_words_and_labels(df)\n"],"metadata":{"id":"HJifJstjIAeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Invert the id2label mapping to create a label2id mapping\n","label2id = {label: id for id, label in id2label.items()}\n","\n","\n","# Corrected conversion function that can handle a list of lists\n","def convert_labels_to_integers(label_series, mapping):\n","     \"\"\"\n","    Converts a series of label lists to their corresponding integer IDs using the provided mapping.\n","\n","    :param label_series: Pandas Series where each element is a list of labels.\n","    :param mapping: Dictionary mapping labels to integers.\n","    :return: List of lists with integer labels.\n","    \"\"\"\n","    integer_labels = []\n","    for labels in label_series:\n","        int_label_row = []\n","        for label in labels:\n","            # Check if the label is in the mapping, otherwise ignore it\n","            if label in mapping:\n","                int_label_row.append(mapping[label])\n","        integer_labels.append(int_label_row)\n","    return integer_labels\n","\n","# Apply the function to the 'cleaned_labels' column with the new mapping\n","df['integer_labels'] = convert_labels_to_integers(df['cleaned_labels'], label2id)\n","\n","df[['cleaned_words', 'integer_labels']]\n"],"metadata":{"id":"LTX3wczOAJ8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final = df[['cleaned_words', 'integer_labels']]"],"metadata":{"id":"dSb_hyUdBh2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final.rename(columns={'integer_labels': 'cleaned_labels'}, inplace=True)"],"metadata":{"id":"mjqHPVitBmU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final"],"metadata":{"id":"e7vtKiNLBuao"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Split data into training, testing and validation sets**"],"metadata":{"id":"UziEuIaM0QN6"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Then you can use the train_test_split function\n","train_val_df, test_df = train_test_split(final, test_size=0.1, random_state=42)\n","train_df, val_df = train_test_split(train_val_df, test_size=(1/9), random_state=42)\n"],"metadata":{"id":"dXSZlllQOsIm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df"],"metadata":{"id":"YfktnvGTOsC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_df"],"metadata":{"id":"MthhPll_PE4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_df"],"metadata":{"id":"rLZCv3R8PLkY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Insert [CLS] and [SEP] Tokens for Transformer Models**"],"metadata":{"id":"jRf7_ptmb6Wj"}},{"cell_type":"code","source":["def insert_cls_sep(df):\n","     \"\"\"\n","    Inserts [CLS] at the beginning and [SEP] at the end of each sentence within the cleaned words of a DataFrame.\n","    Adds [SEP] after periods to signify end of sentence and starts a new sentence with [CLS] if not the last word.\n","\n","    :param df: DataFrame containing 'cleaned_words' and 'cleaned_labels'.\n","    :return: DataFrame with updated words and labels including [CLS] and [SEP] tokens.\n","    \"\"\"\n","    updated_data = []\n","    for _, row in df.iterrows():\n","        words = row['cleaned_words']\n","        labels = row['cleaned_labels']\n","        new_words = ['[CLS]']\n","        new_labels = [-101]\n","\n","        for i, word in enumerate(words):\n","            # Append the word and its label\n","            new_words.append(word.strip('\"'))  # Remove quotes and add word\n","            new_labels.append(labels[i])\n","\n","            # Check if the word ends with a period or is a period\n","            if word.endswith('.'):\n","                # Append [SEP] after a word that ends with a period\n","                new_words.append('[SEP]')\n","                new_labels.append(-101)\n","\n","                # If this word is not the last word, start a new sentence with [CLS]\n","                if i < len(words) - 1:\n","                    new_words.append('[CLS]')\n","                    new_labels.append(-101)\n","\n","        # Ensure there is a [SEP] at the end of the last sentence if not already there\n","        if new_words[-1] != '[SEP]':\n","            new_words.append('[SEP]')\n","            new_labels.append(-101)\n","\n","        updated_data.append({'cleaned_words': new_words, 'cleaned_labels': new_labels})\n","\n","    return pd.DataFrame(updated_data)\n","\n","# Assume train_df, val_df, and test_df are your DataFrames\n","# You would call the function like this:\n","updated_train_df = insert_cls_sep(train_df)\n","updated_val_df = insert_cls_sep(val_df)\n","updated_test_df = insert_cls_sep(test_df)\n"],"metadata":{"id":"ayhI-HY4utuF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Check [CLS] and [SEP] Token Placement**"],"metadata":{"id":"azEuKBbv0Yhu"}},{"cell_type":"code","source":["def check_cls_sep_tokens(df):\n","     \"\"\"\n","    Prints the placement of [CLS] and [SEP] tokens for the first row of the DataFrame to verify correct insertion.\n","\n","    :param df: DataFrame with tokenized data including special tokens.\n","    \"\"\"\n","    first_row_words = df.iloc[0]['cleaned_words']\n","    first_row_labels = df.iloc[0]['cleaned_labels']\n","    assert len(first_row_words) == len(first_row_labels), \"Mismatch in lengths of tokens and labels\"\n","\n","    for token, label in zip(first_row_words, first_row_labels):\n","        print(f\"{token} ({label})\", end=' ')\n","\n","    print(\"\\n\\nCheck if [CLS] and [SEP] with -101 labels are correctly placed:\")\n","    print(\"Start of sentence:\", first_row_words[0], first_row_labels[0])\n","    print(\"End of sentence:\", first_row_words[-1], first_row_labels[-1])\n","\n","    # Check if [CLS] and [SEP] are correctly added after each period\n","    for i in range(1, len(first_row_words)-1):\n","        if first_row_words[i] == '.':\n","            print(\"Period detected at position\", i)\n","            print(\"Should be [SEP] next:\", first_row_words[i+1], first_row_labels[i+1])\n","            print(\"Should be [CLS] after [SEP] if not last:\", first_row_words[i+2] if i+2 < len(first_row_words) else \"N/A\",\n","                  first_row_labels[i+2] if i+2 < len(first_row_labels) else \"N/A\")\n","\n","# Use the function on your DataFrame\n","check_cls_sep_tokens(updated_train_df)\n"],"metadata":{"id":"LaGmaXA2AG-E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Count Sentences Per Row**"],"metadata":{"id":"qL54bnPq0ku8"}},{"cell_type":"code","source":["def count_sentences_per_row(df):\n","    # Count the number of sentences based on [CLS] tokens\n","    df['sentence_count'] = df['cleaned_words'].apply(lambda words: words.count('[CLS]'))\n","    return df\n","\n","#Call the function on the updated_train_df DataFrame\n","updated_train_df = count_sentences_per_row(updated_train_df)\n","\n","\n","#To see the sentence count for each row, you can now print or inspect the 'sentence_count' column:\n","print(updated_train_df['sentence_count'])\n"],"metadata":{"id":"XKoOlBP5DXe6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Call the function on the updated_train_df DataFrame\n","updated_test_df = count_sentences_per_row(updated_test_df)\n","\n","\n","#To see the sentence count for each row, you can now print or inspect the 'sentence_count' column:\n","print(updated_test_df['sentence_count'])"],"metadata":{"id":"cD_KCHlxECeT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#Call the function on the updated_train_df DataFrame\n","updated_val_df = count_sentences_per_row(updated_val_df)\n","\n","\n","#To see the sentence count for each row, you can now print or inspect the 'sentence_count' column:\n","print(updated_val_df['sentence_count'])"],"metadata":{"id":"yDq2jBkfEJlO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Selecting only 'cleaned_words' and 'cleaned_labels' columns for train, test, and validation sets\n","train = updated_train_df[['cleaned_words', 'cleaned_labels']]\n","test = updated_test_df[['cleaned_words', 'cleaned_labels']]\n","val = updated_val_df[['cleaned_words', 'cleaned_labels']]\n"],"metadata":{"id":"i0mW0GbvEisl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train"],"metadata":{"id":"3oGmpboDEtG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test"],"metadata":{"id":"IpvTz6oIE17k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val"],"metadata":{"id":"ZCi-ruTdGrL8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Sliding Window Tokenization Setup**"],"metadata":{"id":"JjXmPTsx0pmn"}},{"cell_type":"code","source":["from transformers import BertTokenizerFast\n","import torch\n","\n","# Initialize the tokenizer\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n","\n","def sliding_window_tokenization_and_labels(words, labels, max_len=512, slide_len=256):\n","    tokenized_inputs = {\n","        \"input_ids\": [],\n","        \"attention_mask\": [],\n","        \"labels\": []\n","    }\n","\n","    # Process each sequence with sliding windows\n","    start_index = 0\n","    while start_index < len(words):\n","        end_index = start_index + slide_len\n","        window_words = words[start_index:end_index]\n","        window_labels = labels[start_index:end_index]\n","\n","        # Tokenization and padding to max length\n","        inputs = tokenizer(\n","            window_words,\n","            is_split_into_words=True,\n","            add_special_tokens=False,  # We already have [CLS] and [SEP] in our sequence\n","            max_length=max_len,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        # Align labels with the tokenized word IDs, adding -101 for ignored tokens\n","        word_ids = inputs.word_ids(0)  # Batch index 0 since we're processing one sequence at a time\n","        window_aligned_labels = [-101 if word_id is None else window_labels[word_id] for word_id in word_ids]\n","\n","        # Append the tokenized results\n","        tokenized_inputs['input_ids'].append(inputs['input_ids'].squeeze(0))\n","        tokenized_inputs['attention_mask'].append(inputs['attention_mask'].squeeze(0))\n","        tokenized_inputs['labels'].append(torch.tensor(window_aligned_labels, dtype=torch.long))\n","\n","        # Move start index to the next slide\n","        start_index += slide_len\n","\n","    # Stack all the tensors\n","    tokenized_inputs['input_ids'] = torch.stack(tokenized_inputs['input_ids'])\n","    tokenized_inputs['attention_mask'] = torch.stack(tokenized_inputs['attention_mask'])\n","    tokenized_inputs['labels'] = torch.stack(tokenized_inputs['labels'])\n","\n","    return tokenized_inputs\n"],"metadata":{"id":"bewOTlN0GsCP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Data Preparation for Training, Testing, and Validation**"],"metadata":{"id":"tymT3Y9q0vWs"}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Initialize lists to hold the tokenized data for all rows\n","train_input_ids = []\n","train_attention_masks = []\n","train_labels = []\n","\n","# Example usage for each row in your DataFrame\n","\n","\n","for index, row in train.iterrows():\n","    tokenized_train_data = sliding_window_tokenization_and_labels(\n","        row['cleaned_words'], row['cleaned_labels']\n","    )\n","    train_input_ids.append(tokenized_train_data['input_ids'])\n","    train_attention_masks.append(tokenized_train_data['attention_mask'])\n","    train_labels.append(tokenized_train_data['labels'])\n","\n","\n","# Now concatenate the lists of tensors into single tensors\n","train_input_ids = torch.cat(train_input_ids, dim=0)\n","train_attention_masks = torch.cat(train_attention_masks, dim=0)\n","train_labels = torch.cat(train_labels, dim=0)\n","\n","# Convert tokenized data into a TensorDataset\n","train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n","\n","# Define a batch size\n","batch_size = 16  # You can adjust this according to your requirements\n","\n","# Create a DataLoader\n","train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=batch_size)\n"],"metadata":{"id":"f9wRKgqyzkKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize lists to hold the tokenized data for all rows\n","test_input_ids = []\n","test_attention_masks = []\n","test_labels = []\n","\n","# Tokenize all the data in the test set\n","for index,row in test.iterrows():\n","    tokenized_test_data = sliding_window_tokenization_and_labels(\n","        row['cleaned_words'], row['cleaned_labels']\n","    )\n","    test_input_ids.append(tokenized_test_data['input_ids'])\n","    test_attention_masks.append(tokenized_test_data['attention_mask'])\n","    test_labels.append(tokenized_test_data['labels'])\n","\n","\n","# Concatenate the lists of tensors into single tensors\n","test_input_ids = torch.cat(test_input_ids, dim=0)\n","test_attention_masks = torch.cat(test_attention_masks, dim=0)\n","test_labels = torch.cat(test_labels, dim=0)\n","\n","# Convert the tokenized test data into a TensorDataset\n","test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n","\n","# For test DataLoader, we usually don't need to shuffle the data, so we use the SequentialSampler\n","test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n"],"metadata":{"id":"7LQt3y5hzkH5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize lists to hold the tokenized data for all rows\n","val_input_ids = []\n","val_attention_masks = []\n","val_labels = []\n","\n","# Tokenize all the data in the validation set\n","for index, row in val.iterrows():\n","    tokenized_val_data = sliding_window_tokenization_and_labels(\n","        row['cleaned_words'], row['cleaned_labels']\n","    )\n","    val_input_ids.append(tokenized_val_data['input_ids'])\n","    val_attention_masks.append(tokenized_val_data['attention_mask'])\n","    val_labels.append(tokenized_val_data['labels'])\n","\n","# Concatenate the lists of tensors into single tensors\n","val_input_ids = torch.cat(val_input_ids, dim=0)\n","val_attention_masks = torch.cat(val_attention_masks, dim=0)\n","val_labels = torch.cat(val_labels, dim=0)\n","\n","# Convert the tokenized validation data into a TensorDataset\n","val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n","\n","# # For validation DataLoader, we usually don't need to shuffle the data either\n","# val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=batch_size)\n","# When initializing your validation dataloader, make sure to set shuffle to False\n","val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n"],"metadata":{"id":"y-vNKZsBzkFv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["id2label"],"metadata":{"id":"KutCjnsqlgoe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Verification of Tokenization and Label Alignment**"],"metadata":{"id":"f1_yAgja1KZL"}},{"cell_type":"code","source":["def verify_tokenization_and_labels(tokenizer, input_ids, attention_masks, labels, id2label, num_examples=3):\n","    \"\"\"\n","    Prints tokens and their corresponding labels from the tokenized data to verify correct processing.\n","\n","    :param tokenizer: Tokenizer used to convert token IDs back to tokens.\n","    :param input_ids: Tensor of token IDs.\n","    :param attention_masks: Tensor of attention masks (not used directly in printing).\n","    :param labels: Tensor of label IDs.\n","    :param id2label: Dictionary mapping label IDs to label names.\n","    :param num_examples: Number of examples to display for verification.\n","    \"\"\"\n","    for i in range(num_examples):\n","        # Get the tokens and label IDs for the current example\n","        tokens = tokenizer.convert_ids_to_tokens(input_ids[i])\n","        label_ids = labels[i]\n","\n","        # Now convert the label IDs to label names using id2label, excluding the ignored indices (-101)\n","        label_names = [id2label[label_id.item()] if label_id != -101 else 'IGN' for label_id in label_ids]\n","\n","        # Print the tokens and their labels\n","        print(\"{:15} {:5}\".format(\"Token\", \"Label\"))\n","        for token, label_name in zip(tokens, label_names):\n","            print(\"{:15} {:5}\".format(token, label_name))\n","        print(\"\\n\")\n","\n","# Assuming id2label is a dictionary that maps label indices to label names\n","# Assuming train_input_ids, train_attention_masks, train_labels are the tensors from your tokenized dataset\n","\n","# Call the function to verify the tokenization and labels\n","verify_tokenization_and_labels(tokenizer,test_input_ids,test_attention_masks, test_labels, id2label)\n"],"metadata":{"id":"LCsREaX6lS0a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_input_ids))\n","print(len(train_attention_masks))\n","print(len(train_labels))\n","\n","print(len(test_input_ids))\n","print(len(test_attention_masks))\n","print(len(test_labels))\n","\n","print(len(val_input_ids))\n","print(len(val_attention_masks))\n","print(len(val_labels))"],"metadata":{"id":"bRV4vMgtLRgH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"jdMLkDyHBDwK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Setup for Weighted Loss Function in PyTorch**"],"metadata":{"id":"D4tkfrFB1THR"}},{"cell_type":"code","source":["import torch\n","\n","# Define the frequency counts of labels\n","label_freqs = {\n","    'O': 5929765,  # The 'O' or 'Outside' class\n","    'B-NAME_STUDENT': 12385,\n","    'I-STREET_ADDRESS': 8551,\n","    'I-NAME_STUDENT': 6732,\n","    'B-EMAIL': 3811,\n","    'B-STREET_ADDRESS': 3468,\n","    'I-PHONE_NUM': 3375,\n","    'B-PHONE_NUM': 2409,\n","    'B-URL_PERSONAL': 730,\n","    'B-USERNAME': 724,\n","    'B-ID_NUM': 78,\n","    'I-URL_PERSONAL': 1,\n","    'I-ID_NUM': 1\n","}\n"],"metadata":{"id":"2rpzoituBDpT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import  torch\n","import torch.nn as nn\n","\n","# Define the frequency counts of labels\n","# Calculate weights as the inverse of the frequency\n","weights = 1.0 / torch.tensor(list(label_freqs.values()), dtype=torch.float)\n","\n","# Normalize the weights so that the most common class ('O') gets a weight of 1\n","weights = weights / weights[0]  # Assuming 'O' is the first class\n","\n","# Move the weights to the device\n","weights = weights.to(device)\n","\n","# Your loss function now uses these custom weights\n","loss_fn = nn.CrossEntropyLoss(weight=weights, ignore_index=-101)\n"],"metadata":{"id":"KWlTNvwgDXAI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for label, weight in zip(id2label.values(), weights):\n","    print(f\"{label}: {weight:.4f}\")"],"metadata":{"id":"Oe1rxyhkDPey"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**BERT Model Setup for Token Classification Training**"],"metadata":{"id":"EE_b6Ylm1YGu"}},{"cell_type":"code","source":["from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n","import torch.nn as nn\n","# Parameters\n","\n","num_labels = len(id2label)\n","batch_size = 16\n","num_epochs = 50  # Higher number, since early stopping can halt training\n","learning_rate = 5e-5\n","\n","\n","model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)\n","model.to(device)\n","model.train()\n","\n","\n","# Optimizer and Scheduler\n","optimizer = AdamW(model.parameters(), lr=learning_rate)\n","total_steps = len(train_dataloader) * num_epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","# Loss function\n","loss_fn = nn.CrossEntropyLoss(weight=weights, ignore_index=-101).to(device)\n","\n","# Early Stopping setup\n","patience = 3  # Number of epochs to wait after last time validation loss improved.\n","best_val_loss = float('inf')\n","best_accuracy = 0\n","no_improve_epochs = 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["2f0540be86564ebda3556b6ff7d75015","69f46b465e3c45ba86f89ec4deabd062","0554de5fdd814e3ca7518eaa1b62c949","15d05d65bbed49db825ef9551b0ab460","e3aa326f6d034214880a7763a3de5726","41e6588343d54e8db50b5fb846ddb0ab","5dd62462f8984995a6c383ec79360aad","9a66d0737d3b465994677c69b8bf9f09","f105797de38e435e989f2a118fe65bf7","b220e5beacd9487bbe28a12b69c3134c","e2145a19dd2c4abc80a07cb635624c0e"]},"id":"jTZO2yG_W48_","outputId":"512f5e3c-2158-42cb-e6d8-54adcb4249cd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f0540be86564ebda3556b6ff7d75015"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["**BERT Model Training and Validation Loop**"],"metadata":{"id":"u4dqwhjT1ezB"}},{"cell_type":"code","source":["import torch\n","from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n","import numpy as np\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","import torch.nn as nn\n","from sklearn.metrics import classification_report\n","import random\n","\n","# Function to calculate accuracy per batch\n","def batch_accuracy(logits, labels):\n","    # Get the predictions and compare with true labels\n","    preds = torch.argmax(logits, dim=-1)\n","    mask = labels != -101  # Exclude the -101 labels from calculation\n","    corrects = (preds == labels) & mask  # Correct predictions\n","    accuracy = corrects.sum().item() / mask.sum().item()\n","    return accuracy\n","\n","\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# Learning Rate Adjustment Check\n","def get_current_learning_rate(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","\n","\n","# Begin training loop\n","best_val_loss = float('inf')\n","best_accuracy = 0\n","no_improve_epochs = 0\n","\n","initial_model_params = [p.clone() for p in model.parameters()]\n","\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    print(f\"Current Learning Rate: {get_current_learning_rate(optimizer)}\")\n","    total_loss = 0\n","    total_accuracy = 0\n","\n","    model.train()\n","\n","    for batch in train_dataloader:\n","        input_ids, attention_masks, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_masks = attention_masks.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_masks)\n","\n","        loss = loss_fn(outputs.logits.view(-1, num_labels), labels.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        total_loss += loss.item()\n","\n","        # Calculate batch accuracy from the logits and labels\n","        batch_acc = batch_accuracy(outputs.logits, labels)\n","        total_accuracy += batch_acc\n","\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    avg_train_accuracy = total_accuracy / len(train_dataloader)\n","    print(f'Epoch {epoch + 1}, Train Loss: {avg_train_loss}, Train Accuracy: {avg_train_accuracy}')\n","\n","\n","\n","    model.eval()\n","    val_loss = 0\n","    val_accuracy = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            input_ids, attention_masks, labels = batch\n","            input_ids = input_ids.to(device)\n","            attention_masks = attention_masks.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(input_ids, attention_mask=attention_masks)\n","\n","            loss = loss_fn(outputs.logits.view(-1, num_labels), labels.view(-1))\n","            val_loss += loss.item()\n","\n","            # Calculate batch accuracy from the logits and labels\n","            batch_acc = batch_accuracy(outputs.logits, labels)\n","            val_accuracy += batch_acc\n","\n","    avg_val_loss = val_loss / len(val_dataloader)\n","    avg_val_accuracy = val_accuracy / len(val_dataloader)\n","    print(f'Epoch {epoch + 1}, Validation Loss: {avg_val_loss}, Validation Accuracy: {avg_val_accuracy}')\n","\n","    # Check if current epoch's validation loss is the best we've seen so far.\n","    if avg_val_loss < best_val_loss:\n","        torch.save(model.state_dict(), '/content/drive/My Drive/ML-Project/PII-DATA/best_model_state.bin')\n","        best_val_loss = avg_val_loss\n","        best_accuracy = avg_val_accuracy\n","        no_improve_epochs = 0\n","    else:\n","        no_improve_epochs += 1\n","        if no_improve_epochs >= patience:\n","            print(f\"Early stopping triggered at epoch {epoch + 1}.\")\n","            print(f\"Best Val Loss: {best_val_loss}, Best Val Accuracy: {best_accuracy}\")\n","            break\n"],"metadata":{"id":"OZVDf71kwo2A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"db894cc5-0eec-4803-9d1a-30f4a8595ef4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Current Learning Rate: 5e-05\n","Epoch 1, Train Loss: 0.21626304347535827, Train Accuracy: 0.8564273596302984\n","Epoch 1, Validation Loss: 0.06742244782090841, Validation Accuracy: 0.9746162973736396\n","Current Learning Rate: 4.9e-05\n","Epoch 2, Train Loss: 0.06386199768780489, Train Accuracy: 0.9687090862489101\n","Epoch 2, Validation Loss: 0.09918512586755336, Validation Accuracy: 0.9082165842121732\n","Current Learning Rate: 4.8e-05\n","Epoch 3, Train Loss: 0.09388274302631155, Train Accuracy: 0.898032409671692\n","Epoch 3, Validation Loss: 0.04015657188281227, Validation Accuracy: 0.967814084955008\n","Current Learning Rate: 4.7e-05\n","Epoch 4, Train Loss: 0.06909297279181446, Train Accuracy: 0.9306840482577585\n","Epoch 4, Validation Loss: 0.047053603628066404, Validation Accuracy: 0.9046064145134589\n","Current Learning Rate: 4.600000000000001e-05\n","Epoch 5, Train Loss: 0.06445915794788526, Train Accuracy: 0.9316675125571409\n","Epoch 5, Validation Loss: 0.027897994319697434, Validation Accuracy: 0.9529702428196197\n","Current Learning Rate: 4.5e-05\n","Epoch 6, Train Loss: 0.050605808533752066, Train Accuracy: 0.9588275711676907\n","Epoch 6, Validation Loss: 0.052339692786655935, Validation Accuracy: 0.9658844353698713\n","Current Learning Rate: 4.4000000000000006e-05\n","Epoch 7, Train Loss: 0.04445794802003077, Train Accuracy: 0.9549207699278406\n","Epoch 7, Validation Loss: 0.04905820247348147, Validation Accuracy: 0.9665274879922968\n","Current Learning Rate: 4.3e-05\n","Epoch 8, Train Loss: 0.06121327372054502, Train Accuracy: 0.9081868434460225\n","Epoch 8, Validation Loss: 0.05829553315412931, Validation Accuracy: 0.9503527050945042\n","Early stopping triggered at epoch 8.\n","Best Val Loss: 0.027897994319697434, Best Val Accuracy: 0.9529702428196197\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/My Drive/ML-Project/PII-DATA/best_model_state.bin')"],"metadata":{"id":"73PxDLOjWwxH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**BERT Model Test Evaluation and Classification Report**"],"metadata":{"id":"mj_b1Qap1lCg"}},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","import numpy as np\n","\n","# Prepare lists to accumulate true labels and predictions\n","true_labels_list = []\n","predictions_list = []\n","\n","# Load the best model state\n","model.load_state_dict(torch.load('/content/drive/My Drive/ML-Project/PII-DATA/best_model_state.bin', map_location=device))\n","model.eval()  # Set the model to evaluation mode\n","\n","test_loss = 0\n","test_accuracy = 0\n","\n","with torch.no_grad():  # No gradients needed\n","    for batch in test_dataloader:\n","        input_ids, attention_masks, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_masks = attention_masks.to(device)\n","        labels = labels.to(device)\n","\n","        outputs = model(input_ids, attention_mask=attention_masks)\n","        loss = loss_fn(outputs.logits.view(-1, num_labels), labels.view(-1))\n","        test_loss += loss.item()\n","\n","        # Calculate batch accuracy from the logits and labels\n","        batch_acc = batch_accuracy(outputs.logits, labels)\n","        test_accuracy += batch_acc\n","\n","        # Get the true labels and predictions\n","        preds = torch.argmax(outputs.logits, dim=-1)\n","        true_labels_list.extend(labels.view(-1).cpu().numpy())\n","        predictions_list.extend(preds.view(-1).cpu().numpy())\n","\n","# Compute the average loss and accuracy\n","avg_test_loss = test_loss / len(test_dataloader)\n","avg_test_accuracy = test_accuracy / len(test_dataloader)\n","\n","# Remove the ignored index (-101) from true labels and predictions\n","true_labels = [label for label in true_labels_list if label != -101]\n","predictions = [pred for pred, true in zip(predictions_list, true_labels_list) if true != -101]\n","\n","# Print results\n","print(f'Test Loss: {avg_test_loss}')\n","print(f'Test Accuracy: {avg_test_accuracy}')\n","\n","# Generate and print the classification report\n","report = classification_report(true_labels, predictions, labels=list(id2label.keys()), target_names=list(id2label.values()), zero_division=0)\n","print(report)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhXzIMA_J9FF","outputId":"8a99d91e-8e02-4bbc-930c-19a281d7e7f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.04134382714300486\n","Test Accuracy: 0.9506893999747704\n","                  precision    recall  f1-score   support\n","\n","               O       1.00      0.95      0.97    638713\n","  B-NAME_STUDENT       0.34      0.97      0.51      2292\n","  I-NAME_STUDENT       0.50      0.98      0.66      1444\n","  B-URL_PERSONAL       0.32      0.99      0.48      1173\n","         B-EMAIL       0.66      0.98      0.78      4016\n","        B-ID_NUM       0.00      0.00      0.00        30\n","  I-URL_PERSONAL       0.00      0.00      0.00         6\n","      B-USERNAME       0.37      0.79      0.50       362\n","     B-PHONE_NUM       0.44      0.98      0.61       803\n","     I-PHONE_NUM       0.72      0.94      0.82       971\n","B-STREET_ADDRESS       0.07      0.89      0.13       636\n","I-STREET_ADDRESS       0.09      1.00      0.17      1313\n","        I-ID_NUM       0.00      0.00      0.00         0\n","\n","       micro avg       0.95      0.95      0.95    651759\n","       macro avg       0.35      0.73      0.43    651759\n","    weighted avg       0.99      0.95      0.97    651759\n","\n"]}]}]}